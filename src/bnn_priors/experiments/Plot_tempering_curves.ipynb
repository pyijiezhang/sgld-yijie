{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the tempering curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import seaborn as sns\n",
    "import json\n",
    "import torch\n",
    "\n",
    "from bnn_priors.exp_utils import load_samples\n",
    "\n",
    "sns.set(context=\"paper\", style=\"white\", font_scale=1.8)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put in the name of your experiment here\n",
    "exp_name = \"my_experiment\"\n",
    "\n",
    "# Set this to True if you ran evaluations using eval_bnn.py\n",
    "# Otherwise, if you just want to use the evaluations that ran with the training, set it to False\n",
    "use_eval_runs = True\n",
    "\n",
    "# Choose an experiment type from [\"mnist\", \"fashion_mnist\", \"cifar10\"]\n",
    "exp_type = \"mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We assume that your experiments are saved in ../results\n",
    "# If that is not the case, you'll have to change it here\n",
    "train_files = f\"../results/{exp_name}/*/config.json\"\n",
    "eval_files = f\"../results/{exp_name}/*/eval/*/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_eval_runs:\n",
    "    files = eval_files\n",
    "else:\n",
    "    files = train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exp_type == \"mnist\":\n",
    "    calibration_data = \"rotated_mnist\"\n",
    "    ood_data = \"fashion_mnist\"\n",
    "elif exp_type == \"fashion_mnist\":\n",
    "    calibration_data = \"fashion_mnist\"\n",
    "    ood_data = \"mnist\"\n",
    "elif exp_type == \"cifar10\":\n",
    "    calibration_data = \"cifar10c\"\n",
    "    ood_data = \"svhn\"\n",
    "else:\n",
    "    raise ValueError(f\"Unknown experiment type {exp_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the priors we used in our paper\n",
    "monolithic_priors = [\"gaussian\", \"convcorrnormal\", \"laplace\", \"student-t\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tempering_curve(runs, y, yerr=None, ylabel=\"performance\", ylim=None, x=\"weight_prior\",\n",
    "              title=None, baseline=None, baseline_err=None, log_x=True, legend=True, legend_loc=\"best\",\n",
    "                        invert_y=False):\n",
    "    \"\"\"This function plots the tempering curve of y for different curves x.\"\"\"\n",
    "    scales = sorted(runs.weight_scale.unique())\n",
    "    temps = sorted(runs.temperature.unique())\n",
    "    \n",
    "    if 0. in temps:\n",
    "        temps.remove(0.)\n",
    "        \n",
    "    \n",
    "    fig, axes = plt.subplots(len(scales), 1, sharex=True, figsize=(3*2+2,3*len(scales)+2))\n",
    "    \n",
    "    if len(scales) == 1:\n",
    "        axes = [axes]\n",
    "        \n",
    "    for scale, ax in zip(scales, axes):\n",
    "        for x_val in runs.sort_values([x], ascending=False)[x].unique():\n",
    "            df = runs.sort_values([x, \"weight_scale\", \"temperature\"]).query(f\"weight_scale == {scale} & {x} == '{x_val}'\")\n",
    "            if len(df[\"temperature\"].unique()) != len(df[\"temperature\"]):\n",
    "                df_stderr = df.groupby(by=\"temperature\").apply(lambda group: group.std() / np.sqrt(len(group)))[[y]]\n",
    "                df_mean = df.groupby(by=\"temperature\").mean()\n",
    "                df_mean[f\"{y}_stderr\"] = df_stderr[y]\n",
    "                df = df_mean\n",
    "                yerr = f\"{y}_stderr\"\n",
    "                df.reset_index(level=0, inplace=True)\n",
    "            df.plot(x=\"temperature\", y=y, kind=\"line\", legend=legend, ax=ax, label=x_val, linewidth=3)\n",
    "            if yerr is not None:\n",
    "                ax.fill_between(df[\"temperature\"], df[y] - df[yerr], df[y] + df[yerr], alpha=0.3)\n",
    "            # ax.set_title(f\"scale={scale}\")\n",
    "        ax.set_ylabel(ylabel)\n",
    "        if ylim is not None:\n",
    "            ax.set_ylim(ylim)\n",
    "        if baseline is not None:\n",
    "            ax.axhline(y=baseline, color=\"gray\", linestyle=\"dashed\", label=\"SGD\", linewidth=2)\n",
    "            if baseline_err is not None:\n",
    "                ax.fill_between(df[\"temperature\"], baseline-baseline_err, baseline+baseline_err, color=\"gray\", alpha=0.3)\n",
    "        if legend:\n",
    "            plt.legend(frameon=False, loc=legend_loc)\n",
    "        if log_x:\n",
    "            ax.set(xscale=\"log\")\n",
    "        if invert_y:\n",
    "            ax.invert_yaxis()\n",
    "        ax.set_xlim(df[\"temperature\"].min(), df[\"temperature\"].max())\n",
    "                \n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "        fig.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    else:\n",
    "        fig.tight_layout()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs = []\n",
    "for config_file in glob(files):\n",
    "    with open(config_file) as infile:\n",
    "        config = pd.Series(json.load(infile))\n",
    "    with open(config_file[:-11] + \"run.json\") as infile:\n",
    "        result = pd.Series(json.load(infile)[\"result\"], dtype=np.float32)\n",
    "    run_data = pd.concat([config, result])\n",
    "    runs.append(run_data)\n",
    "    if not use_eval_runs and run_data[\"weight_prior\"] == \"improper\":\n",
    "        print(run_data[\"weight_prior\"], run_data[\"temperature\"], run_data[\"weight_scale\"], config_file)\n",
    "runs_all = pd.concat(runs, axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not \"acc_mean\" in runs_all.columns:\n",
    "    runs_all[\"acc_mean\"] = runs_all[\"acc_ensemble\"]\n",
    "\n",
    "if not \"lp_mean\" in runs_all.columns:\n",
    "    runs_all[\"lp_mean\"] = runs_all[\"lp_ensemble\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_all[\"error_mean\"] = 1. - runs_all[\"acc_mean\"]\n",
    "runs_all[\"nll_mean\"] = - runs_all[\"lp_mean\"]\n",
    "runs_all[\"neg_auroc\"] = - runs_all[\"auroc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in runs_all.columns:\n",
    "    runs_all[col] = pd.to_numeric(runs_all[col], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out the failed runs\n",
    "runs_all = runs_all[runs_all[\"acc_mean\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to use the nicer label \"correlated\" in the plots instead of \"convcorrnormal\"\n",
    "runs_all.replace(\"convcorrnormal\", \"correlated\", inplace=True)\n",
    "monolithic_priors = [\"gaussian\", \"correlated\", \"laplace\", \"student-t\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load SGD baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you ran SGD baselines with train_sgd.py you can load the results in here\n",
    "# otherwise don't run these cells\n",
    "sgd_runs = pd.read_pickle(\"../results/4.1_sgd_runs.pkl.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_runs[\"result.error_ensemble\"] = 1. - sgd_runs[\"result.acc_ensemble\"]\n",
    "sgd_runs[\"result.nll_ensemble\"] = - sgd_runs[\"result.lp_ensemble\"]\n",
    "sgd_runs[\"ood.neg_auroc\"] = - sgd_runs[\"ood.auroc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgd_results(model_type, data, measure):\n",
    "    results = sgd_runs.query(f\"model == '{model_type}' and data == '{data}'\")[measure]\n",
    "    mean = results.mean()\n",
    "    stderr = results.std() / np.sqrt(len(results))\n",
    "    return mean, stderr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tempering curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_eval_runs:\n",
    "    runs_selected = runs_all.query(\"eval_data != eval_data\")  # basically checks for None\n",
    "else:\n",
    "    runs_selected = runs_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_subselected = runs_selected.query(f\"weight_prior in {monolithic_priors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, just run this if you have an SGD baseline\n",
    "sgd_mean, sgd_stderr = get_sgd_results(model_type=runs_all[\"model\"].iloc[0], data=runs_all[\"data\"].iloc[0], measure=\"result.error_ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have an SGD baseline, remove the last two arguments\n",
    "fig = plot_tempering_curve(runs_subselected, y=\"error_mean\", ylabel=\"error\", legend=True, baseline=sgd_mean, baseline_err=sgd_stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"../figures/{exp_name}_acc_tempering_curve.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.axes[0].set_title(\"\")\n",
    "fig.axes[0].legend(frameon=False, labelspacing=0.2)\n",
    "fig.set_size_inches(5,3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../figures/{exp_name}_acc_tempering_curve_small.pdf\", bbox_inches = 'tight', pad_inches = 0.1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, just run this if you have an SGD baseline\n",
    "sgd_mean, sgd_stderr = get_sgd_results(model_type=runs_all[\"model\"].iloc[0], data=runs_all[\"data\"].iloc[0], measure=\"result.nll_ensemble\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have an SGD baseline, remove the last two arguments\n",
    "fig = plot_tempering_curve(runs_subselected, y=\"nll_mean\", ylabel=\"NLL\", legend=True, baseline=sgd_mean, baseline_err=sgd_stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"../figures/{exp_name}_nll_tempering_curve.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.axes[0].set_title(\"\")\n",
    "fig.axes[0].legend(frameon=False, labelspacing=0.2)\n",
    "fig.set_size_inches(5,3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../figures/{exp_name}_nll_tempering_curve_small.pdf\", bbox_inches = 'tight', pad_inches = 0.1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_selected = runs_all[runs_all['eval_data'].str.contains(calibration_data, na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_subselected = runs_selected.query(f\"weight_prior in {monolithic_priors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, just run this if you have an SGD baseline\n",
    "sgd_mean, sgd_stderr = get_sgd_results(model_type=runs_all[\"model\"].iloc[0], data=runs_all[\"data\"].iloc[0], measure=\"calibration.ece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have an SGD baseline, remove the last two arguments\n",
    "fig = plot_tempering_curve(runs_subselected, y=\"ece\", ylabel=\"ECE\", legend=True, baseline=sgd_mean, baseline_err=sgd_stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"../figures/{exp_name}_ece_tempering_curve.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.axes[0].set_title(\"\")\n",
    "fig.axes[0].legend(frameon=False, labelspacing=0.2)\n",
    "fig.set_size_inches(5,3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../figures/{exp_name}_ece_tempering_curve_small.pdf\", bbox_inches = 'tight', pad_inches = 0.1)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_selected = runs_all.query(f\"'{ood_data}' in eval_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_subselected = runs_selected.query(f\"weight_prior in {monolithic_priors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, just run this if you have an SGD baseline\n",
    "sgd_mean, sgd_stderr = get_sgd_results(model_type=runs_all[\"model\"].iloc[0], data=runs_all[\"data\"].iloc[0], measure=\"ood.auroc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you don't have an SGD baseline, remove the last two arguments\n",
    "fig = plot_tempering_curve(runs_subselected, y=\"auroc\", ylabel=\"OOD AUROC\", legend=True, invert_y=True, baseline=sgd_mean, baseline_err=sgd_stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.savefig(f\"../figures/{exp_name}_ood_auroc_tempering_curve.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.axes[0].set_title(\"\")\n",
    "fig.axes[0].legend(frameon=False, labelspacing=0.2)\n",
    "fig.set_size_inches(5,3)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../figures/{exp_name}_ood_auroc_tempering_curve_small.pdf\", bbox_inches = 'tight', pad_inches = 0.1)\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
