{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from scipy import stats\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import gridspec\n",
    "import json\n",
    "import torch\n",
    "import gpytorch\n",
    "import h5py\n",
    "import collections\n",
    "import scipy\n",
    "import torch\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from bnn_priors import prior\n",
    "\n",
    "from bnn_priors.exp_utils import load_samples\n",
    "from bnn_priors.notebook_utils import collect_runs, unique_cols, json_dump, json_load\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.print_figure_kwargs = {'bbox_inches':None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data and plot figures 3, 5 and A.10\n",
    "\n",
    "With the repository we ship the extracted data, necessary to plot the figures. The code for *creating* the CSVs and JSONs is at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_with_eval = pd.read_csv(\"Plot_CIFAR10_resnet_data/sgd_runs.csv\", index_col=0)\n",
    "opt_dfs = json_load(\"Plot_CIFAR10_resnet_data/opt_dfs.json\")\n",
    "opt_lengthscale = json_load(\"Plot_CIFAR10_resnet_data/opt_lengthscale.json\")\n",
    "(covs, lens, conv_n_channels) = pd.read_pickle(\"Plot_CIFAR10_resnet_data/covs_lens.pkl.gz\")\n",
    "\n",
    "conv_keys = list(covs.keys())\n",
    "conv_keys.sort(key=lambda k: (int(k.split('.')[2]), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(9, 2.3), sharex=True)\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_title(\"Degrees of freedom\")\n",
    "ax.plot([opt_dfs[k][1] for k in conv_keys])\n",
    "ax.set_xticks([1, 7, 13, 18])\n",
    "ax.set_xticklabels([\"*L2\", \"*L8\", \"*L14\", \"L19\"])\n",
    "ax.set_yscale('log')\n",
    "\n",
    "ax = axes[0] \n",
    "ax.set_title(\"Lengthscale\")\n",
    "ax.plot([opt_lengthscale[k] for k in conv_keys])\n",
    "\n",
    "fig.text(0.5, 0, 'Layer index (input = L0)', ha='center')\n",
    "#fig.suptitle(\"ResNet-20, CIFAR-10 SGD: fitted parameters for T-distribution and Gaussian\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(\"../figures/210122_resnet_fitted.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 3\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"axes.linewidth\": 0.5,\n",
    "    'ytick.major.width': 0.5,\n",
    "    'xtick.major.width': 0.5,\n",
    "    'ytick.minor.width': 0.5,\n",
    "    'xtick.minor.width': 0.5,\n",
    "    \"figure.dpi\": 300,\n",
    "})\n",
    "fig_width_pt = 234.8775\n",
    "inches_per_pt = 1.0/72.27               # Convert pt to inches\n",
    "fig_width = fig_width_pt*inches_per_pt  # width in inches\n",
    "\n",
    "fig, axes = plt.subplots(1, 1, figsize=(fig_width, 1.3), sharex=True, gridspec_kw=dict(\n",
    "    top=1, bottom=0.34, left=0.17, right=1))\n",
    "\n",
    "ax = axes\n",
    "ax.set_ylabel(\"Deg. of freedom\", horizontalalignment=\"right\", position=(0, 1))\n",
    "ax.plot([opt_dfs[k][1] for k in conv_keys])\n",
    "ax.set_xticks([1, 7, 13, 18])\n",
    "ax.set_xticklabels([\"*L2\", \"*L8\", \"*L14\", \"L19\"])\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('Layer index (input = L0)')\n",
    "\n",
    "fig.savefig(\"../figures/210126-resnet-dof.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure 5\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": False,\n",
    "    \"font.family\": \"sans-serif\"})\n",
    "\n",
    "fig_width_pt = 487.8225\n",
    "inches_per_pt = 1.0/72.27               # Convert pt to inches\n",
    "fig_width = fig_width_pt*inches_per_pt  # width in inches\n",
    "\n",
    "mean_covs = {k: covs[k]/lens[k] * conv_n_channels[k] for k in conv_keys}\n",
    "\n",
    "plots_x = 7\n",
    "plots_y = 3\n",
    "\n",
    "margins = dict(\n",
    "    left=0.015,\n",
    "    right=0.01,\n",
    "    top=0.007,\n",
    "    bottom=0.02)\n",
    "\n",
    "wsep = hsep = 0.002\n",
    "w_cov_sep = 0.02\n",
    "h_cov_sep = 0.03\n",
    "height = width = (1 - w_cov_sep*(plots_x-1) - wsep*3*plots_x\n",
    "         - margins['left'] - margins['right'])/plots_x / 3\n",
    "ttl_marg=5\n",
    "\n",
    "fig_height_mult = (margins['bottom'] + (height*3 + hsep*2)*plots_y + h_cov_sep*plots_y + margins['top'])\n",
    "\n",
    "# make figure rectangular and correct vertical sizes\n",
    "hsep /= fig_height_mult\n",
    "height /= fig_height_mult\n",
    "h_cov_sep /= fig_height_mult\n",
    "margins['bottom'] /= fig_height_mult\n",
    "margins['top'] /= fig_height_mult\n",
    "fig = plt.figure(figsize=(fig_width, fig_width *fig_height_mult))\n",
    "\n",
    "print(\"fig height = \", fig_width *fig_height_mult)\n",
    "\n",
    "cbar_height = height*3 + hsep*2\n",
    "\n",
    "\n",
    "extreme = max(*(mean_covs[k].abs().max().item() for k in mean_covs.keys()))  #1.68\n",
    "#assert extreme < 1.7\n",
    "extreme = 2\n",
    "norm = Normalize(-extreme, extreme)\n",
    "    \n",
    "def plot_at(key, base_bottom, base_left, is_bottom_row=False, is_left_col=False, title=\"title\"):\n",
    "    max_bottom = base_bottom\n",
    "    max_left = base_left\n",
    "    \n",
    "    for y in range(3):\n",
    "        for x in range(3):\n",
    "            bottom = base_bottom + (height+hsep) * (2-y)\n",
    "            left = base_left + (width+wsep) * x\n",
    "            max_bottom = max(max_bottom, bottom+height+hsep)\n",
    "            max_left = max(max_left, left+width+wsep)\n",
    "            \n",
    "            if x == 0 and is_left_col:\n",
    "                yticks = [1, 2, 3]\n",
    "            else:\n",
    "                yticks = []\n",
    "\n",
    "            if (y == 2 and is_bottom_row) or title==\"Layer 15\":\n",
    "                xticks = [1, 2, 3]\n",
    "            else:\n",
    "                xticks = []\n",
    "            ax = fig.add_axes([left, bottom, width, height], xticks=xticks, yticks=yticks)\n",
    "                              #title=f\"cov. w/ ({x + 1}, {y +1})\")\n",
    "            extreme = 1\n",
    "            mappable = ax.imshow(\n",
    "                mean_covs[key][y*3+x, :].reshape((3, 3)) / mean_covs[key].abs().max().item() ,\n",
    "                cmap=plt.get_cmap('RdBu'),\n",
    "                extent=[0.5, 3.5, 3.5, 0.5], norm=Normalize(-extreme, extreme))\n",
    "            ax.plot([x+1], [y+1], marker='x', ls='none', color=('white' if title == \"Layer 19\" else 'white'),\n",
    "                   ms=3, markeredgewidth=0.5)\n",
    "            ax.tick_params(left=False, bottom=False, labelsize=\"xx-small\", pad=0)  # remove ticks\n",
    "\n",
    "            if y==0 and x==1:\n",
    "                ttl = ax.set_title(title, pad=ttl_marg, size=\"x-small\")\n",
    "    return max_bottom, max_left, mappable\n",
    "\n",
    "# Iterate over the indices for axes, starting from the bottom-left of the plots\n",
    "cur_bottom = margins['bottom']\n",
    "for y_idx in reversed(range(0, plots_y)):\n",
    "    cur_left = margins['left']\n",
    "    for x_idx in range(min(len(conv_keys)-y_idx*plots_x, plots_x)):\n",
    "        key = conv_keys[y_idx*plots_x + x_idx]\n",
    "        if key in ['net.module.3.main.0.weight_prior.p',\n",
    "                   'net.module.6.main.0.weight_prior.p',\n",
    "                   'net.module.9.main.0.weight_prior.p',]:\n",
    "            marker = \"*\"\n",
    "        else:\n",
    "            marker = \"\"\n",
    "        \n",
    "        \n",
    "        next_bottom, cur_left, mappable = plot_at(\n",
    "            key, cur_bottom, cur_left,\n",
    "            is_bottom_row=(y_idx == plots_y-1),\n",
    "            is_left_col=(x_idx == 0),\n",
    "            title=f\"{marker}Layer {conv_keys.index(key) + 1}\")\n",
    "        cur_left += w_cov_sep\n",
    "        \n",
    "    if cur_bottom == margins[\"bottom\"]:\n",
    "        cbar_width = width/3\n",
    "        cbar_ax = fig.add_axes([cur_left, cur_bottom, cbar_width, next_bottom-cur_bottom])\n",
    "        fig.colorbar(mappable, cax=cbar_ax, ticks=[-extreme, -1, 0, 1, extreme])\n",
    "        \n",
    "        # plot absolute variance\n",
    "        bottom = cur_bottom+0.02\n",
    "        lmarg = 2.6667*width + 2*wsep\n",
    "        ax = fig.add_axes([cur_left+lmarg, bottom,\n",
    "                           1-(cur_left+lmarg + margins[\"right\"] ), next_bottom-bottom])\n",
    "        ax.set_ylabel(\"Max. variance\", size=\"x-small\") #, horizontalalignment=\"right\", position=(0, 1))\n",
    "        ax.plot([mean_covs[k].abs().max().item() for k in mean_covs.keys()])\n",
    "        ax.set_xticks([1, 7, 13, 18])\n",
    "        ax.set_xticklabels([\"*L2\", \"*L8\", \"*L14\", \"L19\"])\n",
    "        ax.set_ylim((0, 6))\n",
    "        #ax.set_yscale('log')\n",
    "        #ax.set_xlabel('Layer index')\n",
    "        ax.tick_params(labelsize=\"x-small\")\n",
    "\n",
    "        \n",
    "        cbar_ax.tick_params(labelsize=\"x-small\")\n",
    "    cur_bottom = next_bottom + h_cov_sep\n",
    "    \n",
    "fig.savefig(\"../figures/210204_googleresnet_covariances_all_capped.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise RuntimeError(\"Do you want to continue?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect SGD Runs for various data sets\n",
    "\n",
    "Here we will only read the relevant CSV file. The cells enclosed in `if False` below were used to create it.\n",
    "\n",
    "You need to run `jug/0_31_googleresnet_cifar10_sgd.py` to be able to run the following.\n",
    "\n",
    "Run `eval_bnn.py` and construct the overall dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = collect_runs(\"../logs/0_31_googleresnet_cifar10_sgd\")\n",
    "\n",
    "good_runs = df[(df[\"n_epochs\"] == 600) & (df[\"status\"] == \"COMPLETED\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_bnn(**config):\n",
    "    args = [sys.executable, \"eval_bnn.py\", \"with\",\n",
    "                               *[f\"{k}={v}\" for k, v in config.items()]]\n",
    "    print(\" \".join(args))\n",
    "    complete = subprocess.run(args)\n",
    "    if complete.returncode != 0:\n",
    "        raise SystemError(f\"Process returned with code {complete.returncode}\")\n",
    "\n",
    "#for i, (_, run) in enumerate(good_runs.iterrows()):\n",
    "if False:  # This would run eval_bnn.py on the relevant directory. Only needs to be run once.\n",
    "    print(f\"run {i}/{len(good_runs)}\")\n",
    "    config_file = str(run[\"the_dir\"]/\"config.json\")\n",
    "    \n",
    "    calibration_data = {\n",
    "        \"mnist\": \"rotated_mnist\",\n",
    "        \"fashion_mnist\": \"fashion_mnist\",\n",
    "        \"cifar10\": \"cifar10c-gaussian_blur\",\n",
    "        \"cifar10_augmented\": \"cifar10c-gaussian_blur\",\n",
    "    }[run[\"data\"]]\n",
    "    \n",
    "    eval_bnn(is_run_sgd=True, calibration_eval=True, eval_data=calibration_data,\n",
    "             config_file=config_file, skip_first=2, batch_size=128)\n",
    "    \n",
    "    ood_data = {\n",
    "        \"mnist\": \"fashion_mnist\",\n",
    "        \"fashion_mnist\": \"mnist\",\n",
    "        \"cifar10\": \"svhn\",\n",
    "        \"cifar10_augmented\": \"svhn\",\n",
    "    }[run[\"data\"]]    \n",
    "    \n",
    "    eval_bnn(is_run_sgd=True, ood_eval=True, eval_data=ood_data,\n",
    "             config_file=config_file, skip_first=2, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_with_eval = []\n",
    "\n",
    "for _, run in good_runs.iterrows():\n",
    "    corresponding = collect_runs(run[\"the_dir\"]/\"eval\", metrics_must_exist=False)\n",
    "\n",
    "    new_run = [run]\n",
    "    for _, corr in corresponding.iterrows():\n",
    "        orig_keys = [k for k in corr.index if k.startswith(\"result.\")]\n",
    "        if corr[\"calibration_eval\"]:\n",
    "            purpose = \"calibration\"\n",
    "            assert not corr[\"ood_eval\"]\n",
    "        elif corr[\"ood_eval\"]:\n",
    "            purpose = \"ood\"\n",
    "        else:\n",
    "            raise ValueError(\"unknown purpose\")\n",
    "        new_keys = [k.replace(\"result.\", purpose+\".\") for k in orig_keys]\n",
    "        for k in new_keys:\n",
    "            assert k not in run.index\n",
    "\n",
    "        new_corr = corr[orig_keys]\n",
    "        new_corr.index = new_keys\n",
    "        new_run.append(new_corr)\n",
    "    runs_with_eval.append(pd.concat(new_run))\n",
    "runs_with_eval = pd.DataFrame(runs_with_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the lengthscales and df's from each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_weights(df):\n",
    "    samples = collections.defaultdict( lambda: [], {})\n",
    "    for _, row in df.iterrows():\n",
    "        try:\n",
    "            s = load_samples(row[\"the_dir\"]/\"samples.pt\", idx=-1, keep_steps=False)\n",
    "        except pickle.UnpicklingError:\n",
    "            continue\n",
    "        assert len(samples.keys()) == 0 or set(s.keys()) == set(samples.keys())\n",
    "        for k in s.keys():\n",
    "            samples[k].append(s[k])\n",
    "    return {k: torch.stack(v, dim=0) for k, v in samples.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = collect_weights(good_runs[good_runs[\"data\"] == \"cifar10_augmented\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 16, 3, 3, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples['net.module.0.weight_prior.p'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in samples.keys():\n",
    "    if k.endswith(\".p\"):\n",
    "        print(k, tuple(samples[k].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_keys = [\"net.module.0.weight_prior.p\", *filter(\n",
    "    lambda k: k.endswith(\".p\") and \"main\" in k, samples.keys())]\n",
    "conv_keys.sort(key=lambda k: (int(k.split('.')[2]), k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "covs = {}\n",
    "lens = {}\n",
    "for k in conv_keys:\n",
    "    M = samples[k].view(-1, 3*3)\n",
    "    covs[k] = (M.t() @ M)\n",
    "    lens[k] = len(M)\n",
    "conv_n_channels = {k: samples[k].size(-3) for k in conv_keys}\n",
    "    \n",
    "pd.to_pickle((covs, lens, conv_n_channels), \"4.1_covs_lens.pkl.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = torch.from_numpy(np.mgrid[:3, :3].reshape(2, -1).T).contiguous().to(torch.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import math\n",
    "torch.set_default_dtype(torch.float64)\n",
    "kern = gpytorch.kernels.RBFKernel(batch_shape=torch.Size([1000]))\n",
    "kern.lengthscale = torch.linspace(0.001**.5, 30**.5, 1000).unsqueeze(-1).pow(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_inverse = kern(points).inv_matmul(torch.eye(9))\n",
    "S_logdet = kern(points).logdet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_liks = {}\n",
    "opt_lengthscale = {}\n",
    "for k in covs.keys():\n",
    "    with torch.no_grad():\n",
    "        log_liks[k] = S_logdet.mul(lens[k] / -2) - 0.5 * S_inverse.mul(covs[k]).sum((-2, -1))\n",
    "        opt_lengthscale[k] = kern.lengthscale[torch.argmax(log_liks[k])].item()\n",
    "json_dump(opt_lengthscale, \"4.1_opt_lengthscale.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = next(iter(log_liks.keys()))\n",
    "\n",
    "plt.plot(kern.lengthscale.squeeze(-1).detach(), log_liks[k])\n",
    "plt.ylim((-10000, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(conv_keys)), [opt_lengthscale[k] for k in conv_keys])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-5513.4573, grad_fn=<SumBackward0>), tensor(-5513.4573))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that log-likelihoods aren't buggy\n",
    "dist = gpytorch.distributions.MultivariateNormal(torch.zeros(9), kern[100](points))\n",
    "dist.log_prob(samples[k].view(-1, 9)).sum(), log_liks[k][100] - math.log(2*math.pi) * 9 * lens[k]/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net.module.0.weight_prior.p': 0.6403542406466662,\n",
       " 'net.module.3.main.0.weight_prior.p': 1.0704316857604412,\n",
       " 'net.module.3.main.3.weight_prior.p': 1.0931095467878191,\n",
       " 'net.module.4.main.0.weight_prior.p': 1.1508441889397887,\n",
       " 'net.module.4.main.3.weight_prior.p': 1.1625694012987868,\n",
       " 'net.module.5.main.0.weight_prior.p': 1.036860604040884,\n",
       " 'net.module.5.main.3.weight_prior.p': 1.32040510780009,\n",
       " 'net.module.6.main.0.weight_prior.p': 1.3079073317504737,\n",
       " 'net.module.6.main.3.weight_prior.p': 1.3582550038061472,\n",
       " 'net.module.7.main.0.weight_prior.p': 1.409553523481043,\n",
       " 'net.module.7.main.3.weight_prior.p': 1.58284050379471,\n",
       " 'net.module.8.main.0.weight_prior.p': 1.5150031056885025,\n",
       " 'net.module.8.main.3.weight_prior.p': 1.7086917828865713,\n",
       " 'net.module.9.main.0.weight_prior.p': 1.6521636013059522,\n",
       " 'net.module.9.main.3.weight_prior.p': 1.4225267233402705,\n",
       " 'net.module.10.main.0.weight_prior.p': 1.5016139099958647,\n",
       " 'net.module.10.main.3.weight_prior.p': 1.680308836143859,\n",
       " 'net.module.11.main.0.weight_prior.p': 1.8246006889054769,\n",
       " 'net.module.11.main.3.weight_prior.p': 2.1790248672256287}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_lengthscale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get max df of multivariate-T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MVTFitter(torch.nn.Module):\n",
    "    def __init__(self, p, df, permute=None, event_dim=2):\n",
    "        flat_p = p.view(-1, 9)\n",
    "        cov = (flat_p.t() @ flat_p) / len(flat_p)\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dist = prior.MultivariateT(\n",
    "            p.size(), torch.zeros(9), cov.cholesky().detach().to(torch.get_default_dtype()),\n",
    "            df=torch.nn.Parameter(torch.tensor(df, requires_grad=True)),\n",
    "            event_dim=event_dim, permute=permute)\n",
    "        \n",
    "        self.dist.p.requires_grad_(False)\n",
    "        self.dist.p[...] = p\n",
    "        \n",
    "    def closure(self):\n",
    "        self.zero_grad()\n",
    "        lp = -self.dist.log_prob()\n",
    "        lp.backward()\n",
    "        return lp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "opt_dfs = {}\n",
    "\n",
    "try_df_inits = torch.linspace(math.log(2.1), math.log(1000), 300).exp()\n",
    "\n",
    "for key in conv_keys:\n",
    "    max_lik = -np.inf\n",
    "    \n",
    "    for permute, event_dim in [(None, 2), (None, 3), (None, 4), ((0, 2, 1, 3, 4), 3)]:\n",
    "        mvt = MVTFitter(samples[key], 3., permute=permute, event_dim=event_dim).cuda()\n",
    "        for df_init in try_df_inits:\n",
    "            with torch.no_grad():\n",
    "                mvt.dist.df[...] = df_init\n",
    "\n",
    "            lik = mvt.dist.log_prob().item()\n",
    "            df = mvt.dist.df.item()\n",
    "            if np.isnan(lik) or np.isnan(df):\n",
    "                print(\"key\", key, \"saw a nan with lik\", lik)\n",
    "\n",
    "            if lik > max_lik:\n",
    "                opt_dfs[key] = (lik, df, (permute, event_dim))\n",
    "                max_lik = lik\n",
    "json_dump(opt_dfs, \"4.1_opt_dfs.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore degrees of freedom of MNIST weights\n",
    "\n",
    "You need to run `jug/0_12_mnist_no_weight_decay.py` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df of MVT in MNIST\n",
    "\n",
    "mnist_weights = collections.defaultdict( lambda: [], {})\n",
    "for i in range(8):\n",
    "    samples_file = f\"../logs/sgd-no-weight-decay/mnist_classificationconvnet/{i}/samples.pt\"\n",
    "    s = load_samples(samples_file)\n",
    "    for k in s.keys():\n",
    "        if k.endswith(\".p\"):\n",
    "            mnist_weights[k].append(s[k][-1])\n",
    "mnist_weights = {k: torch.stack(v, 0) for (k, v) in mnist_weights.items()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_conv_keys = ['net.module.1.weight_prior.p', 'net.module.4.weight_prior.p', 'net.module.8.weight_prior.p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net.module.1.weight_prior.p': (-1663.31383774297,\n",
       "  107.83875426199431,\n",
       "  (None, 2)),\n",
       " 'net.module.4.weight_prior.p': (461357.6454892001,\n",
       "  3.5897961780383376,\n",
       "  (None, 2)),\n",
       " 'net.module.8.weight_prior.p': (None, 1.4334627049114599, None)}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_mnist_dfs = {}\n",
    "\n",
    "try_df_inits = torch.linspace(math.log(2.1), math.log(1000), 300).exp()\n",
    "\n",
    "for key in mnist_conv_keys:\n",
    "    max_lik = -np.inf\n",
    "    \n",
    "    for permute, event_dim in [(None, 2), (None, 3), (None, 4), ((0, 2, 1, 3, 4), 3)]:\n",
    "        try:\n",
    "            mvt = MVTFitter(mnist_weights[key], 3., permute=permute, event_dim=event_dim).cuda()\n",
    "            for df_init in try_df_inits:\n",
    "                with torch.no_grad():\n",
    "                    mvt.dist.df[...] = df_init\n",
    "\n",
    "                lik = mvt.dist.log_prob().item()\n",
    "                df = mvt.dist.df.item()\n",
    "                if np.isnan(lik) or np.isnan(df):\n",
    "                    print(\"key\", key, \"saw a nan with lik\", lik)\n",
    "\n",
    "                if lik > max_lik:\n",
    "                    opt_mnist_dfs[key] = (lik, df, (permute, event_dim))\n",
    "                    max_lik = lik\n",
    "        except RuntimeError as e:\n",
    "            dist = scipy.stats.t.fit(mnist_weights[key].numpy())\n",
    "            opt_mnist_dfs[key] = (None, dist[0], None)\n",
    "            \n",
    "opt_mnist_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'net.module.0.weight_prior.p': 2.1691800464281084,\n",
       " 'net.module.2.weight_prior.p': 6.292621228487064,\n",
       " 'net.module.4.weight_prior.p': 11.825714788660914}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df of MVT in MNIST\n",
    "\n",
    "fcnn_weights = collections.defaultdict( lambda: [], {})\n",
    "for i in range(10):\n",
    "    if i ==5 :\n",
    "        continue\n",
    "    samples_file = f\"../logs/sgd-no-weight-decay/mnist_classificationdensenet/{i}/samples.pt\"\n",
    "    s = load_samples(samples_file)\n",
    "    for k in s.keys():\n",
    "        if k.endswith(\"weight_prior.p\"):\n",
    "            fcnn_weights[k].append(s[k][-1])\n",
    "fcnn_weights = {k: torch.stack(v, 0) for (k, v) in fcnn_weights.items()}\n",
    "\n",
    "{k: scipy.stats.t.fit(v)[0] for k, v in fcnn_weights.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
